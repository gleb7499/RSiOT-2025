# Лекция 20. Сеть Kubernetes и сервис‑меш (Service Mesh) для начинающих

## Цель
Понять, как устроена сеть в Kubernetes (адресация, Service, DNS, CNI), научиться ограничивать трафик через NetworkPolicy и осознать зачем нужен Service Mesh: прозрачная безопасность (mTLS), маршрутизация (canary, A/B, mirroring), наблюдаемость (метрики, трассировка), управление сбоями (fault injection, retry, circuit breaking). Рассмотрим Istio / Linkerd, их архитектуру и применение, а также подходы multi‑cluster и ambient mesh.

## План
1. Базовая модель сети Kubernetes.
2. CNI плагины: Calico, Cilium, Flannel (вкратце сравнение).
3. Service и DNS: ClusterIP, NodePort, LoadBalancer, Ingress.
4. NetworkPolicy: default deny, ingress/egress ограничения.
5. Зачем Service Mesh: проблемы без него.
6. Архитектура Mesh: data plane (Envoy) + control plane (Istiod).
7. Sidecar vs Ambient Mesh (перспектива развития).
8. mTLS и идентичность (SPIFFE / SPIRE).
9. Управление трафиком: виртуальные сервисы, маршрутизация по версии.
10. Observability: метрики, логирование, distributed tracing.
11. Надёжность: retry, timeout, fault injection, circuit breaking.
12. Политики и безопасность: AuthorizationPolicy, Rate Limiting.
13. Multi‑cluster и failover.
14. Практическое задание.
15. Вопросы для самопроверки.

---

## 1. Сеть Kubernetes: ключевые идеи
Каждый Pod получает IP‑адрес в плоском сетевом пространстве: "Pod‑to‑Pod communication without NAT" (основной принцип). Контейнеры внутри Pod'а делят один network namespace → общие интерфейсы и localhost.

```plaintext
Node
 ├─ Pod A (IP: 10.42.0.11)
 ├─ Pod B (IP: 10.42.0.12)
 └─ Pod C (IP: 10.42.0.13)

Другой Node
 ├─ Pod D (IP: 10.42.1.5)
```

Маршрутизация между узлами реализуется CNI плагином. Kubernetes DNS (CoreDNS) даёт имена сервисам: `my-service.my-namespace.svc.cluster.local`.

## 2. CNI плагины (обзор)
| Плагин | Особенности | Когда выбрать |
|--------|-------------|---------------|
| Flannel | Простая overlay сеть (VXLAN) | Быстрый старт, простота |
| Calico | L3 маршрутизация, NetworkPolicy, BGP | Гибкая политика, производительность |
| Cilium | eBPF для сетевого стека, observability | Высокая прозрачность, производительность, advanced security |

eBPF (Cilium) позволяет собирать метрики/трассировку без сайдкаров.

## 3. Типы Service и Ingress
| Тип | Назначение | Пример |
|-----|-----------|--------|
| ClusterIP | Доступ внутри кластера | Бэкенд API между сервисами |
| NodePort | Проброс порта на ноды | Быстрая отладка, нестабильно |
| LoadBalancer | Внешний балансировщик (Cloud) | Продакшн входящий трафик |
| Headless (clusterIP=None) | DNS записи на Pod'ы | StatefulSet (шардирование) |

Ingress — контроллер уровня HTTP (Nginx, Traefik, Istio Gateway) для маршрутизации на разные сервисы по хосту/пути.

## 4. NetworkPolicy
Без политики – всё разрешено (default allow). NetworkPolicy позволяет ограничить:
- Какие Pod'ы могут подключаться к другим (ingress).
- Куда Pod может ходить (egress).

Пример default deny + разрешить трафик только от метки `app=frontend`:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
  namespace: prod
spec:
  podSelector:
    matchLabels: { app: backend }
  policyTypes: [Ingress]
  ingress:
  - from:
    - podSelector:
        matchLabels: { app: frontend }
```
-
## 5. Зачем Service Mesh? (Боль без него)

Без mesh каждая команда встраивает в код:

- Логирование и метрики (Prometheus клиент, OpenTelemetry SDK).
- Трейсинг (headers propagation).
- Аутентификация между сервисами (TLS сертификаты).
- Ретраи, timeouts, circuit breaker.
- Сложное канареечное распределение трафика.

Это дублирование логики во всех языках и сервисах → больше ошибок, сложнее поддержка.

Service Mesh переносит эти cross‑cutting concerns в инфраструктуру (плоскость данных с прокси) + централизованный контроль (плоскость управления).

## 6. Архитектура Mesh (Istio пример)

```plaintext
             +------------------+
             |  Control Plane   |
             |  Istiod          |
             +---------+--------+
                       |
                Push config (xDS)
                       |
  +--------+     +-----+-----+     +--------+
  | Service|     |  Sidecar  |     | Service|
  |   A    |<--->|  Envoy    |<--->|   B    |
  +--------+     +-----------+     +--------+
```
Data Plane: Sidecar proxy (Envoy) перехватывает входящий/исходящий трафик. Control Plane: формирует конфигурации маршрутизации, политики безопасности.

### 7.1 Как sidecar перехватывает трафик (под капотом)
Механика перехвата основана на iptables (или eBPF‑hook'ах у некоторых решений) и «прокладке» трафика через Envoy.

- Init‑контейнер (или привилегированный агент) настраивает iptables‑правила в netns Pod'а.
- Пробы здоровья Kubernetes могут быть переписаны, чтобы работать при STRICT mTLS.

Ключевые порты Envoy (по умолчанию в Istio):

- 15006 — inbound listener (перехват входящих соединений к приложению)
- 15001 — outbound listener (весь исходящий трафик)
- 15021 — статус/пробы sidecar (HTTP)
- 15000 — admin интерфейс Envoy (локальный)

Исключения и диапазоны управляются аннотациями Pod'а:

```yaml
metadata:
  annotations:
    sidecar.istio.io/inject: "true"                # Явная инъекция сайдкара
    sidecar.istio.io/rewriteAppHTTPProbers: "true"  # Переписать liveness/readiness пробы через прокси
    traffic.sidecar.istio.io/includeOutboundIPRanges: "0.0.0.0/0"  # Что перехватывать наружу
    traffic.sidecar.istio.io/excludeOutboundIPRanges: "10.96.0.0/12" # Исключить, если нужно
    traffic.sidecar.istio.io/excludeOutboundPorts: "5432,6379"        # Не проксировать DB/Redis и т.п.
    traffic.sidecar.istio.io/excludeInboundPorts: "15020,15021"       # Исключить служебные порты
```

Практический совет: базы данных, длительные TCP‑соединения или кастомные протоколы иногда целесообразно исключить из проксирования (миграционно или постоянно).
```

### 7.2 Bootstrap и конфигурация Envoy

- Sidecar запускается с минимальным bootstrap‑конфигом и подключается к Control Plane (Istiod) по xDS (gRPC) для получения полной конфигурации: маршруты, кластеры, политики.
- Сертификаты для mTLS sidecar получает через SDS (Secret Discovery Service) от локального агента (istio‑agent), который общается с CA.
- Список конечных точек (EDS) синхронизируется с Kubernetes EndpointSlice — прокси знает актуальные Pod IP/порт для каждого сервиса.
- В Envoy создаются listeners/filters:
  - HTTP Connection Manager, RBAC/JWT фильтры, router.
  - Кластеры вида `outbound|80||api.default.svc.cluster.local` для маршрутизации по хосту/порту/протоколу.

### 7.3 Ресурсы и тюнинг

Sidecar добавляет накладные расходы:

- Память: ориентировочно 50–150 МБ на Pod (зависит от числа listeners/маршрутов).
- CPU: рост латентности p99 на единицы‑десятки миллисекунд под нагрузкой.

Рекомендации:

- Задавайте ресурсы через аннотации:

```yaml
metadata:
  annotations:
    sidecar.istio.io/proxyCPU: "100m"
    sidecar.istio.io/proxyMemory: "128Mi"
    proxy.istio.io/config: |
      { "concurrency": 2 }    # Число воркеров Envoy (ядра)
```
- Группируйте маршрутизацию и минимизируйте число уникальных VirtualService/DestinationRule.
- Используйте HTTP/2/gRPC для лучшей эффективности соединений.

 
### 7.4 Failure modes и как их избежать
- Sidecar не готов → Pod не принимает трафик. Решение: используйте ReadinessGate (добавляется инжектором) и `rewriteAppHTTPProbers: true`.
- STRICT mTLS ломает пробы/внешние вызовы → перепишите пробы через прокси, временно PERMISSIVE при миграции.
- Потеря связи с Istiod/SDS → устаревшие маршруты/сертификаты. Мониторьте `istiod` и sidecar логи, алерты на срок действия сертификатов.
- Неверные exclude/iptables → «blackhole» исходящего трафика. Проверяйте правила и используйте `istioctl analyze`.

 
### 7.5 Отладка и диагностика
Команды (в кластере):

```bash
# Состояние прокси и синхронизация с istiod
istioctl proxy-status

# Просмотр маршрутов/листенеров/кластеров конкретного Pod'а
istioctl proxy-config routes <pod> -n <ns>
istioctl proxy-config listeners <pod> -n <ns>
istioctl proxy-config clusters <pod> -n <ns>

# Полный config dump Envoy
kubectl exec <pod> -c istio-proxy -- curl -s http://localhost:15000/config_dump

# Анализ манифестов и проблем конфигурации
istioctl analyze -n <ns>
```
Linux‑узлы (для сетевых проблем): `iptables -t nat -S | grep 15001`, `tcpdump -i any port 15006`.

 
### 7.6 Паттерны внедрения
- Namespace‑уровень: `kubectl label ns <name> istio-injection=enabled` — массовая инъекция.
- Тонкая настройка на Pod: аннотации include/exclude.
- Пошаговая миграция: сначала PERMISSIVE mTLS, затем STRICT; затем включение AuthorizationPolicy.
- Исключение чувствительных портов (например, `5432`) на первых этапах.

### 7.7 Sidecar vs Ambient — практическая перспектива
- Sidecar: максимум гибкости (per‑pod политика, пер‑сервис маршрутизация), цена — ресурсы и операционная сложность.
- Ambient: проще управление и меньший overhead, но ещё не везде доступен функциональный паритет.

## 8. mTLS и идентичность (Zero Trust внутри кластера)
mTLS (Mutual TLS): клиент и сервер проходят взаимную аутентификацию. Mesh автоматически выдаёт сертификаты сервисам (через CA) и обновляет их.

SPIFFE (Secure Production Identity Framework for Everyone): стандарт формата идентичности (`spiffe://domain/ns/namespace/sa/serviceaccount`). SPIRE — реализация.

Преимущества:
- Шифрование трафика внутри кластера (даже если сеть скомпрометирована).
- Явная идентичность сервисов → более точные политики доступа.

Пример политики авторизации Istio (ограничение по namespace):
```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-frontend
  namespace: backend
spec:
  rules:
    - from:
        - source:
            namespaces: ["frontend"]
  action: ALLOW
```

## 9. Управление трафиком
Istio вводит VirtualService и DestinationRule.

Пример 90% на v1 и 10% на v2:
```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api-routing
spec:
  hosts: ["api"]
  http:
  - route:
    - destination:
        host: api
        subset: v1
      weight: 90
    - destination:
        host: api
        subset: v2
      weight: 10
```

DestinationRule определяет subset:
```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: api-destination
spec:
  host: api
  subsets:
  - name: v1
    labels: { version: v1 }
  - name: v2
    labels: { version: v2 }
```

Fault Injection (симулируем задержку):
```yaml
http:
- fault:
    delay:
      percentage: { value: 10 }
      fixedDelay: 5s
  route: [...]
```

Mirroring (копия трафика):
```yaml
http:
- route: [...]
  mirror:
    host: api
    subset: v2
  mirrorPercentage: { value: 20 }
```

## 10. Observability (Наблюдаемость)
Mesh автоматически собирает:
- Метрики: request count, latency, error rate (Prometheus scraping Envoy).
- Трассировка: заголовки `x-request-id`, `x-b3-*` или W3C traceparent.
- Логи: access logs.

Преимущества: стандартизованный формат, единый экспорт, меньше зависимостей в коде приложения.

## 11. Надёжность и поведение при сбоях
Функции:
- Retry с экспоненциальной задержкой.
- Timeout (завершение зависших запросов).
- Circuit Breaking (лимит одновременных подключений).
- Fault Injection (тест устойчивости).

Пример ограничение подключений:
```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: api-cb
spec:
  host: api
  trafficPolicy:
    connectionPool:
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
```

## 12. Политики и безопасность в Mesh
- AuthorizationPolicy: кто может обращаться.
- PeerAuthentication: уровень TLS (STRICT / PERMISSIVE / DISABLE).
- Rate Limiting: через EnvoyFilter или встроенные расширения.
- Интеграция с OPA: проверка контекстных политик.

PeerAuthentication строгий mTLS:
```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: strict-mtls
spec:
  mtls:
    mode: STRICT
```

## 13. Multi‑cluster и failover
Задача: повысить отказоустойчивость и геораспределение. Istio поддерживает:
- Одна плоскость управления управляет несколькими кластерами.
- Обнаружение сервисов между кластерами (если доверие настроено).

Failover: если подсеть/регион недоступен — трафик перенаправляется.

Риски: сложность, увеличение латентности, необходимость согласованной политики безопасности.

## 14. Практическое задание
1. Разверните простой сервис `api` (v1 и v2) с меткой `version`.
2. Установите Istio (demo profile). Проверьте sidecar injection (`kubectl label namespace default istio-injection=enabled`).
3. Создайте DestinationRule и VirtualService для canary (90/10).
4. Включите STRICT mTLS (PeerAuthentication).
5. Добавьте AuthorizationPolicy — ограничить доступ к `api` из namespace `frontend`.
6. (Опционально) Настройте fault injection: 10% задержка 5 секунд.
7. Соберите метрики через Prometheus / Grafana → постройте график error rate.

Документация сдачи:
- Манифесты (YAML) + краткое описание.
- Скриншот распределения трафика (Grafana или Kiali).
- Вывод `istioctl analyze` без критичных ошибок.

## 15. Вопросы для самопроверки
1. Чем отличается ClusterIP от NodePort?
2. Для чего нужен Headless Service в StatefulSet?
3. Что делает NetworkPolicy и чем опасно её отсутствие?
4. Почему внедрение логики ретраев в код хуже, чем в mesh?
5. Что такое data plane и control plane?
6. Преимущества mTLS внутри кластера?
7. Разница между canary и mirroring?
8. Как fault injection помогает повысить надёжность?
9. Что делает outlierDetection (в DestinationRule)?
10. Какие недостатки у sidecar подхода?
11. Зачем нужен SPIFFE идентификатор?
12. Как происходит маршрутизация 90/10 в VirtualService?
13. В каких случаях mesh — излишняя сложность?
14. Как ограничить доступ к сервису только из одного namespace?
15. Какие компоненты нужны для multi‑cluster mesh?

---
**Итог:** Service Mesh — не волшебство, а способ вынести повторяющиеся сетевые задачи (безопасность, наблюдаемость, управление трафиком) из кода в стандартизованный инфраструктурный слой. Освоив сеть Kubernetes и базовые возможности mesh (маршрутизация, mTLS, политики), вы сможете безопасно и предсказуемо разворачивать распределённые сервисы и уверенно переходить к продвинутым сценариям (multi‑cluster, progressive delivery). 
